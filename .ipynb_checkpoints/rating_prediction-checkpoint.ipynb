{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Product Rating Prediction Using LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          reviewText  overall\n",
      "0  They look good and stick good! I just don't li...        4\n",
      "1  These stickers work like the review says they ...        5\n",
      "2  These are awesome and make my phone look so st...        5\n",
      "3  Item arrived in great time and was in perfect ...        4\n",
      "4  awesome! stays on, and looks great. can be use...        5\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_json('data.json',lines=True)\n",
    "data = data[['reviewText','overall']]\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'abandon' 'ability' ... 'zone' 'UNKNOWN' 'ENDPAD']\n"
     ]
    }
   ],
   "source": [
    "vocab = pd.read_csv('vocab.txt',sep=\" \", header=None)\n",
    "vocab = vocab[0].values\n",
    "vocab = np.append(vocab,\"UNKNOWN\")\n",
    "vocab = np.append(vocab,\"ENDPAD\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3002\n"
     ]
    }
   ],
   "source": [
    "word_map = {}\n",
    "for index,value in enumerate(vocab):\n",
    "    word_map[value] = index\n",
    "n_words = vocab.shape[0]\n",
    "print(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format x_data\n",
    "x_data = []\n",
    "\n",
    "import nltk\n",
    "\n",
    "def get_matrix_ids(s):\n",
    "    id_matrix = []\n",
    "    w = nltk.word_tokenize(s)\n",
    "    w = [i.lower() for i in w if i.isalpha()]\n",
    "    \n",
    "    for i in w:\n",
    "        if i in vocab:\n",
    "            id_matrix.append(word_map[i])\n",
    "        else :\n",
    "            id_matrix.append(word_map[\"UNKNOWN\"]) #Unknown token\n",
    "    return id_matrix\n",
    "for index,row in data.iterrows():\n",
    "    x_data.append(get_matrix_ids(row['reviewText']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mayan\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "x_data = pad_sequences(maxlen=65, sequences=x_data, padding=\"post\", value=n_words - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = pd.get_dummies(data['overall']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2696 3000 2968 1538 2685 2254 3000 2697  801 2697 2538 1815 1202  118\n",
      " 2697 2533 1815 2685 1937 2697 3000 2597 3000  118 3000  376 2379 2688\n",
      " 2957 1715 2431 3001 3001 3001 3001 3001 3001 3001 3001 3001 3001 3001\n",
      " 3001 3001 3001 3001 3001 3001 3001 3001 3001 3001 3001 3001 3001 3001\n",
      " 3001 3001 3001 3001 3001 3001 3001 3001 3001]\n",
      "[0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(x_data[1])\n",
    "print(y_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is ready. For every training point, we hace x as the sequence matrix, and y as its corresponsing one-hot encoded vector for the rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=n_words, output_dim=100, input_length=65))\n",
    "model.add(LSTM(units=100, recurrent_dropout=0.1))\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 65, 100)           300200    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 381,105\n",
      "Trainable params: 381,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 139995 samples, validate on 15556 samples\n",
      "Epoch 1/10\n",
      " - 603s - loss: 1.0800 - acc: 0.5859 - val_loss: 0.9389 - val_acc: 0.6266\n",
      "Epoch 2/10\n",
      " - 580s - loss: 0.9287 - acc: 0.6280 - val_loss: 0.9070 - val_acc: 0.6386\n",
      "Epoch 3/10\n",
      " - 580s - loss: 0.8953 - acc: 0.6409 - val_loss: 0.8934 - val_acc: 0.6376\n",
      "Epoch 4/10\n",
      " - 590s - loss: 0.8713 - acc: 0.6508 - val_loss: 0.8868 - val_acc: 0.6455\n",
      "Epoch 5/10\n",
      " - 625s - loss: 0.8502 - acc: 0.6588 - val_loss: 0.8907 - val_acc: 0.6441\n",
      "Epoch 6/10\n",
      " - 641s - loss: 0.8312 - acc: 0.6680 - val_loss: 0.8911 - val_acc: 0.6454\n",
      "Epoch 7/10\n",
      " - 601s - loss: 0.8126 - acc: 0.6745 - val_loss: 0.8937 - val_acc: 0.6419\n",
      "Epoch 8/10\n",
      " - 581s - loss: 0.7934 - acc: 0.6829 - val_loss: 0.9062 - val_acc: 0.6470\n",
      "Epoch 9/10\n",
      " - 580s - loss: 0.7749 - acc: 0.6906 - val_loss: 0.9117 - val_acc: 0.6385\n",
      "Epoch 10/10\n",
      " - 586s - loss: 0.7573 - acc: 0.6981 - val_loss: 0.9273 - val_acc: 0.6374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2090f466cc0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=30, epochs=10, validation_split=0.1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.94\n",
      "acc: 0.63\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(x_test,y_test, verbose = 2, batch_size = 15)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rate_sentence(s):\n",
    "    w = get_matrix_ids(s)\n",
    "    w = pad_sequences(maxlen=65, sequences=[w], padding=\"post\", value=n_words - 1)\n",
    "    output = model.predict([w])[0]\n",
    "    output = np.argmax(output)\n",
    "    print(\"Predicted rating : \" + str(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted rating : 4\n"
     ]
    }
   ],
   "source": [
    "rate_sentence(\"I love this product so much. I totally reccomend this\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
